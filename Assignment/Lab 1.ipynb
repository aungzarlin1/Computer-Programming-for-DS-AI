{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec324d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "m = X.shape[0]   # number of samples\n",
    "n = X.shape[1]   # number of features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=5)\n",
    "\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "X_train = np.concatenate((intercept, X_train), axis=1) \n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "X_test = np.concatenate((intercept, X_test), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aae2a2",
   "metadata": {},
   "source": [
    "# Task 1 : early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4051ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  30.733337233698883\n",
      "Stop at iteration : 771\n"
     ]
    }
   ],
   "source": [
    "# initialize theta\n",
    "theta = np.zeros((X_train.shape[1]))\n",
    "\n",
    "# define learning_rate\n",
    "alpha = 0.0001\n",
    "\n",
    "# define max_iter\n",
    "max_iter = 100000\n",
    "\n",
    "# set initial previous loss\n",
    "previous_loss = 999999\n",
    "\n",
    "# set threshold\n",
    "tol = 0.0001\n",
    "\n",
    "iter_stop = 0\n",
    "\n",
    "def h_theta(X, theta):\n",
    "    return X @ theta\n",
    "\n",
    "def mse(yhat, y):\n",
    "    return ((yhat - y)**2).sum() / yhat.shape[0]\n",
    "\n",
    "def gradient(X, error):\n",
    "    return X.T @ error\n",
    "\n",
    "for i in range(max_iter):\n",
    "    \n",
    "    #prediction\n",
    "    yhat = h_theta(X_train, theta)\n",
    "     \n",
    "    #current loss\n",
    "    current_loss = mse(yhat, y_train)\n",
    "    \n",
    "    #absolute difference betweeen current loss and previous loss\n",
    "    diff = np.abs(current_loss - previous_loss)\n",
    "    \n",
    "    # if difference is less than threshold, stop\n",
    "    if diff < tol:\n",
    "        break\n",
    "    \n",
    "    # now current loss become prevois loss\n",
    "    previous_loss = current_loss\n",
    "    \n",
    "   \n",
    "    # error\n",
    "    error = yhat - y_train\n",
    "    \n",
    "    #grad\n",
    "    grad = gradient(X_train, error)\n",
    "    \n",
    "    #update theta\n",
    "    theta = theta - alpha * grad\n",
    "    \n",
    "    iter_stop = i\n",
    "    \n",
    "# make prediction\n",
    "yhat = h_theta(X_test, theta)\n",
    "\n",
    "# calculate mean squared errors\n",
    "mse = mse(yhat, y_test)\n",
    "\n",
    "print(\"MSE: \", mse)\n",
    "print(\"Stop at iteration :\", iter_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231f496d",
   "metadata": {},
   "source": [
    "# Task 2 : Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a722099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  32.268643086146575\n",
      "Stop at iter:  55599\n"
     ]
    }
   ],
   "source": [
    "#initialize theta\n",
    "theta = np.zeros((X_train.shape[1]))\n",
    "\n",
    "# define learning_rate\n",
    "alpha = 0.0001\n",
    "\n",
    "#define max_iter\n",
    "max_iter = 100000\n",
    "\n",
    "#set initial previous loss\n",
    "previous_loss = 999999\n",
    "\n",
    "#def threshold\n",
    "tol = 0.0001\n",
    "\n",
    "iter_stop = 0\n",
    "\n",
    "def h_theta(X, theta):\n",
    "    return X @ theta\n",
    "\n",
    "def mse(yhat, y):\n",
    "    return ((yhat - y)**2).sum() / yhat.shape[0]\n",
    "\n",
    "def gradient(X, error):\n",
    "    return X.T @ error\n",
    "\n",
    "random_index = []\n",
    "\n",
    "for i in range(max_iter):\n",
    "    \n",
    "    index = np.random.randint(X_train.shape[0])\n",
    "    while index in random_index:\n",
    "        index = np.random.randint(X_train.shape[0])\n",
    "    \n",
    "    random_index.append(index)\n",
    "    if len(random_index) == X_train.shape[0]:\n",
    "        random_index = []\n",
    "    Xi = X_train[index,:].reshape(1,-1)\n",
    "    yi = y_train[index]\n",
    "    yhat = h_theta(Xi, theta)\n",
    "    current_loss = mse(yhat, yi)\n",
    "    if np.abs(current_loss - previous_loss) < tol:\n",
    "        break\n",
    "    previous_loss = current_loss\n",
    "    error = yhat - yi\n",
    "    grad = gradient(Xi, error)\n",
    "    theta = theta - alpha * grad\n",
    "    \n",
    "    iter_stop = i\n",
    "   \n",
    "    \n",
    "yhat = h_theta(X_test, theta)\n",
    "\n",
    "# calculate mean squared errors\n",
    "mse = mse(yhat, y_test)\n",
    "\n",
    "print(\"MSE: \", mse)\n",
    "print(\"Stop at iter: \", iter_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b934c9ed",
   "metadata": {},
   "source": [
    "# Task 3 : mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3315fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  30.2054782848011\n",
      "stop at iter:  4226\n"
     ]
    }
   ],
   "source": [
    "#initialize theta\n",
    "theta = np.zeros((X_train.shape[1]))\n",
    "\n",
    "# define learning_rate\n",
    "alpha = 0.001\n",
    "\n",
    "#define max_iter\n",
    "max_iter = 100000\n",
    "\n",
    "#set initial previous loss\n",
    "previous_loss = 999999\n",
    "\n",
    "#def threshold\n",
    "tol = 0.0001\n",
    "\n",
    "iter_stop = 0\n",
    "\n",
    "def h_theta(X, theta):\n",
    "    return X @ theta\n",
    "\n",
    "def mse(yhat, y):\n",
    "    return ((yhat - y)**2).sum() / yhat.shape[0]\n",
    "\n",
    "def gradient(X, error):\n",
    "    return X.T @ error\n",
    "\n",
    "for i in range(max_iter):\n",
    "    \n",
    "    # define batch size\n",
    "    size = 10\n",
    "    shuffle_index = np.random.permutation(X_train.shape[0])\n",
    "    X_train = X_train[shuffle_index]\n",
    "    y_train = y_train[shuffle_index]\n",
    "    \n",
    "    for batch_size in range(0, X_train.shape[0], size):\n",
    "        Xi_batch = X_train[batch_size : batch_size + size,:]\n",
    "        yi_batch = y_train[batch_size : batch_size + size]\n",
    "    \n",
    "        #prediction\n",
    "        yhat = h_theta(Xi_batch, theta)\n",
    "        \n",
    "        #current loss\n",
    "        current_loss = mse(yhat, yi_batch)\n",
    "    \n",
    "        #absolute difference betweeen current loss and previous loss\n",
    "        diff = np.abs(current_loss - previous_loss)\n",
    "    \n",
    "        # if difference is less than threshold, stop\n",
    "        if diff < tol:\n",
    "            break\n",
    "    \n",
    "        # now current loss become prevois loss\n",
    "        previous_loss = current_loss\n",
    "    \n",
    "        # error\n",
    "        error = yhat - yi_batch\n",
    "    \n",
    "        #grad\n",
    "        grad = gradient(Xi_batch, error)\n",
    "    \n",
    "        #update theta\n",
    "        theta = theta - alpha * grad\n",
    "    \n",
    "    if diff < tol:\n",
    "        break\n",
    "    \n",
    "    iter_stop = i\n",
    "   \n",
    "# make prediction\n",
    "yhat = h_theta(X_test, theta)\n",
    "\n",
    "# calculate mean squared errors\n",
    "mse = mse(yhat, y_test)\n",
    "\n",
    "print(\"MSE: \", mse)\n",
    "print(\"stop at iter: \", iter_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34acfac8",
   "metadata": {},
   "source": [
    "# Task 4 : put all into class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f99bfeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "m = X.shape[0]  #number of samples\n",
    "n = X.shape[1]  #number of features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=5)\n",
    "\n",
    "# actually you can do like this too\n",
    "# X = np.insert(X, 0, 1, axis=1)\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "X_train = np.concatenate((intercept, X_train), axis=1)\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "X_test = np.concatenate((intercept, X_test), axis=1)\n",
    "\n",
    "class LinearRegression:\n",
    "   \n",
    "    def __init__(self,method=\"batch\", alpha=0.0001, max_iter=100000, previous_loss=99999, tol=0.00001):\n",
    "        self.method = method\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.previous_loss = previous_loss\n",
    "        self.tol = tol\n",
    "     \n",
    "    def h_theta(self, X):\n",
    "        return X @ self.theta\n",
    "\n",
    "    def mse(self, yhat, y):\n",
    "        return ((yhat - y)**2 / yhat.shape[0]).sum()\n",
    "\n",
    "    def gradient(self, X, error):\n",
    "        return X.T @ error\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        random_index = [] \n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            \n",
    "            if self.method == 'batch':\n",
    "                X_train = X\n",
    "                y_train = y\n",
    "                \n",
    "            elif self.method == \"SGD\":\n",
    "                index = np.random.randint(X.shape[0])\n",
    "                while index in random_index:\n",
    "                    index = np.random.randint(X.shape[0])\n",
    "                X_train = X[index, :].reshape(1, -1)\n",
    "                y_train = y[index]\n",
    "                random_index.append(index)\n",
    "                if len(random_index) == X.shape[0]:\n",
    "                    random_index = []\n",
    "            \n",
    "            elif self.method == 'mini-batch':\n",
    "                batch_size = 10\n",
    "                index = np.random.randint(X.shape[0])\n",
    "                X_train = X[index : index + batch_size, :]\n",
    "                y_train = y[index : index + batch_size]\n",
    "\n",
    "            yhat = self.h_theta(X_train)\n",
    "\n",
    "            current_loss = self.mse(yhat, y_train)\n",
    "            if np.abs(current_loss - previous_loss) < tol: \n",
    "                break\n",
    "              \n",
    "            self.previous_loss = current_loss\n",
    "            \n",
    "            error = yhat - y_train\n",
    "            grad = self.gradient(X_train, error)\n",
    "            self.theta = self.theta - self.alpha * grad\n",
    "\n",
    "    \n",
    "    def h_theta(self, X):\n",
    "        return X @ self.theta\n",
    "\n",
    "    def mse(self, yhat, y):\n",
    "        return ((yhat - y)**2 / yhat.shape[0]).sum()\n",
    "\n",
    "    def gradient(self, X, error):\n",
    "        return X.T @ error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60b8d966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE using batch:  30.697037704088505\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression(method='batch')\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.h_theta(X_test)\n",
    "mse = model.mse(yhat, y_test)\n",
    "\n",
    "\n",
    "print(\"MSE using batch: \", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17361187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE using SGD:  31.403373644042464\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression(method='SGD')\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.h_theta(X_test)\n",
    "mse = model.mse(yhat, y_test)\n",
    "\n",
    "\n",
    "print(\"MSE using SGD: \", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bfe024c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE using mini-batch:  30.27920248725819\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression(method='mini-batch')\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.h_theta(X_test)\n",
    "mse = model.mse(yhat, y_test)\n",
    "\n",
    "\n",
    "print(\"MSE using mini-batch: \", mse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonDSAI",
   "language": "python",
   "name": "pythondsai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
