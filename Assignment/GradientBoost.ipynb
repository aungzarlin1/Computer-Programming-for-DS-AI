{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66dce592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X, y =load_boston(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a30a9cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "def grad(y, h):\n",
    "    return y - h\n",
    "\n",
    "def fit(X, y, models):\n",
    "    \n",
    "    models_trained = []\n",
    "    \n",
    "    #starting model, don't use smart model in the first, it will overfit\n",
    "    first_model = DummyRegressor(strategy='mean')\n",
    "    first_model.fit(X, y)\n",
    "    models_trained.append(first_model)\n",
    "    \n",
    "    # fit the estimators\n",
    "    for i, model in enumerate(models):\n",
    "        y_pred = predict(X, models_trained)\n",
    "    \n",
    "        residual = grad(y, y_pred)\n",
    "    \n",
    "        model.fit(X, residual)\n",
    "\n",
    "        models_trained.append(model)\n",
    "    return models_trained\n",
    "\n",
    "def predict(X, models):\n",
    "    learning_rate = 0.1\n",
    "    f0 = models[0].predict(X)\n",
    "    boosting = sum(learning_rate * model.predict(X) for model in models[1:])\n",
    "    return f0 + boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e92d4857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our MSE:  12.945557601580584\n"
     ]
    }
   ],
   "source": [
    "n_estimators = 200\n",
    "tree_params = {'max_depth':1}\n",
    "models = [DecisionTreeRegressor(**tree_params) for _ in range(n_estimators)]\n",
    "\n",
    "models = fit(X_train, y_train, models)\n",
    "\n",
    "y_pred = predict(X_test, models)\n",
    "\n",
    "print(\"Our MSE: \", mean_squared_error(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4b42419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn MSE:  12.945557601580582\n"
     ]
    }
   ],
   "source": [
    "sklearn_model = GradientBoostingRegressor(n_estimators=n_estimators, learning_rate=0.1, max_depth=1, loss='ls')\n",
    "\n",
    "y_pred_sk = sklearn_model.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "print(\"sklearn MSE: \", mean_squared_error(y_test, y_pred_sk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e216dabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:16.15458\n",
      "[1]\tvalidation_0-rmse:11.84377\n",
      "[2]\tvalidation_0-rmse:8.79602\n",
      "[3]\tvalidation_0-rmse:6.72584\n",
      "[4]\tvalidation_0-rmse:5.46526\n",
      "[5]\tvalidation_0-rmse:4.65454\n",
      "[6]\tvalidation_0-rmse:4.08462\n",
      "[7]\tvalidation_0-rmse:3.76129\n",
      "[8]\tvalidation_0-rmse:3.54313\n",
      "[9]\tvalidation_0-rmse:3.37742\n",
      "[10]\tvalidation_0-rmse:3.24836\n",
      "[11]\tvalidation_0-rmse:3.18872\n",
      "[12]\tvalidation_0-rmse:3.10860\n",
      "[13]\tvalidation_0-rmse:3.09993\n",
      "[14]\tvalidation_0-rmse:3.08393\n",
      "[15]\tvalidation_0-rmse:3.08760\n",
      "[16]\tvalidation_0-rmse:3.06310\n",
      "[17]\tvalidation_0-rmse:3.05292\n",
      "[18]\tvalidation_0-rmse:3.05715\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "\n",
    "xgb_reg = xgboost.XGBRegressor()\n",
    "\n",
    "xgb_reg.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=2)\n",
    "y_pred = xgb_reg.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d92191",
   "metadata": {},
   "source": [
    "### Task 1: changing max_depth and min_samples_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bec601d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "max depth:  1 and min samples split: 2\n",
      "Our MSE:  12.945557601580584\n",
      "==============================\n",
      "max depth:  1 and min samples split: 3\n",
      "Our MSE:  12.945557601580582\n",
      "==============================\n",
      "max depth:  1 and min samples split: 4\n",
      "Our MSE:  12.945557601580582\n",
      "==============================\n",
      "max depth:  2 and min samples split: 2\n",
      "Our MSE:  10.78955061410898\n",
      "==============================\n",
      "max depth:  2 and min samples split: 3\n",
      "Our MSE:  10.764879800502445\n",
      "==============================\n",
      "max depth:  2 and min samples split: 4\n",
      "Our MSE:  11.002511681053244\n",
      "==============================\n",
      "max depth:  3 and min samples split: 2\n",
      "Our MSE:  7.604283280819447\n",
      "==============================\n",
      "max depth:  3 and min samples split: 3\n",
      "Our MSE:  7.720158483589702\n",
      "==============================\n",
      "max depth:  3 and min samples split: 4\n",
      "Our MSE:  7.535172299548788\n",
      "==============================\n",
      "max depth:  4 and min samples split: 2\n",
      "Our MSE:  8.301372033899987\n",
      "==============================\n",
      "max depth:  4 and min samples split: 3\n",
      "Our MSE:  8.515133448989273\n",
      "==============================\n",
      "max depth:  4 and min samples split: 4\n",
      "Our MSE:  9.555556825028603\n",
      "==============================\n",
      "max depth:  5 and min samples split: 2\n",
      "Our MSE:  7.531099503536801\n",
      "==============================\n",
      "max depth:  5 and min samples split: 3\n",
      "Our MSE:  8.554678596905008\n",
      "==============================\n",
      "max depth:  5 and min samples split: 4\n",
      "Our MSE:  8.382247247267724\n"
     ]
    }
   ],
   "source": [
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "n_estimators = 200\n",
    "max_depth =[1, 2, 3, 4, 5]\n",
    "min_samples_split = [2, 3, 4]\n",
    "for depth in max_depth:\n",
    "    for mss in min_samples_split:\n",
    "        tree_params = {'max_depth':depth, 'min_samples_split': mss}\n",
    "        models = [DecisionTreeRegressor(**tree_params) for _ in range(n_estimators)]\n",
    "\n",
    "        models = fit(X_train, y_train, models)\n",
    "\n",
    "        y_pred = predict(X_test, models)\n",
    "        \n",
    "        print(\"=\"*30)\n",
    "        print(\"max depth: \", depth, 'and min samples split:', mss)\n",
    "        print(\"Our MSE: \", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c111a8",
   "metadata": {},
   "source": [
    "### Task 2: add code for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff3b3605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "def grad(y, h):\n",
    "    return y - h\n",
    "\n",
    "def fit(X, y, models):\n",
    "    \n",
    "    models_trained = []\n",
    "    \n",
    "    #starting model, don't use smart model in the first, it will overfit\n",
    "    first_model = DummyRegressor(strategy='mean')\n",
    "    first_model.fit(X, y)\n",
    "    models_trained.append(first_model)\n",
    "    \n",
    "    # fit the estimators\n",
    "    for i, model in enumerate(models):\n",
    "        y_pred = predict(X, models_trained)\n",
    "    \n",
    "        residual = grad(y, y_pred)\n",
    "    \n",
    "        model.fit(X, residual)\n",
    "\n",
    "        models_trained.append(model)\n",
    "    return models_trained\n",
    "\n",
    "def predict(X, models):\n",
    "    learning_rate = 0.1\n",
    "    f0 = models[0].predict(X)\n",
    "    boosting = sum(learning_rate * model.predict(X) for model in models[1:])\n",
    "    yhat = f0 + boosting\n",
    "    # sigmoid\n",
    "    y_pred = 1 / (1 + np.exp(-yhat))\n",
    "    y_pred = np.round(y_pred)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d496e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our MSE:  0.8947368421052632\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "n_estimators = 200\n",
    "tree_params = {'max_depth':1}\n",
    "models = [DecisionTreeRegressor(**tree_params) for _ in range(n_estimators)]\n",
    "\n",
    "models = fit(X_train, y_train, models)\n",
    "\n",
    "y_pred = predict(X_test, models)\n",
    "\n",
    "print(\"Our MSE: \", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f379a9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn accuracy:  0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "sklearn_model = GradientBoostingClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    learning_rate = 0.1,\n",
    "    max_depth=1\n",
    ")\n",
    "\n",
    "yhat_sk = sklearn_model.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Sklearn accuracy: \", accuracy_score(y_test, yhat_sk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71954d2",
   "metadata": {},
   "source": [
    "### Task 3 : adding code for multinomial classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56a3644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "def grad(y, h):\n",
    "    return y - h\n",
    "\n",
    "def fit(X, y, models):\n",
    "    \n",
    "    models_trained = []\n",
    "    \n",
    "    #starting model, don't use smart model in the first, it will overfit\n",
    "    first_model = DummyRegressor(strategy='mean')\n",
    "    first_model.fit(X, y)\n",
    "    models_trained.append(first_model)\n",
    "    \n",
    "    # fit the estimators\n",
    "    for i, model in enumerate(models):\n",
    "        y_pred = predict(X, models_trained)\n",
    "    \n",
    "        residual = grad(y, y_pred)\n",
    "    \n",
    "        model.fit(X, residual)\n",
    "\n",
    "        models_trained.append(model)\n",
    "    return models_trained\n",
    "\n",
    "def predict(X, models):\n",
    "    learning_rate = 0.1\n",
    "    f0 = models[0].predict(X)\n",
    "    boosting = sum(learning_rate * model.predict(X) for model in models[1:])\n",
    "    yhat = f0 + boosting\n",
    "    #softmax\n",
    "    y_pred = np.exp(yhat)/np.sum(np.exp(yhat), axis=1,keepdims=True)\n",
    "    #y_pred = np.argmax(y_pred, axis=1)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7b066af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our MSE:  0.8055555555555556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "y_train_encoded = np.zeros((y_train.shape[0], len(set(y))))\n",
    "for each_class in range(len(set(y))):\n",
    "    cond = y_train==each_class\n",
    "    y_train_encoded[np.where(cond), each_class] = 1\n",
    "\n",
    "n_estimators = 200\n",
    "tree_params = {'max_depth':1}\n",
    "models = [DecisionTreeRegressor(**tree_params) for _ in range(n_estimators)]\n",
    "\n",
    "models = fit(X_train, y_train_encoded, models)\n",
    "\n",
    "y_pred = predict(X_test, models)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(\"Our MSE: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ced9082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn accuracy:  0.9481481481481482\n"
     ]
    }
   ],
   "source": [
    "sklearn_model = GradientBoostingClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    learning_rate = 0.1,\n",
    "    max_depth=1\n",
    ")\n",
    "\n",
    "yhat_sk = sklearn_model.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Sklearn accuracy: \", accuracy_score(y_test, yhat_sk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d496bbce",
   "metadata": {},
   "source": [
    "## Put everything into the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4eacc8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "class GradientBoosting:\n",
    "    \n",
    "    def __init__(self, n_estimators=200, max_depth=1, learning_rate=0.1, min_samples_split=2, regression=True):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.regression = regression\n",
    "        \n",
    "        tree_params = {'max_depth' : self.max_depth, 'min_samples_split' : self.min_samples_split}\n",
    "        self.models = [DecisionTreeRegressor(**tree_params) for _ in range(self.n_estimators)]\n",
    "    \n",
    "    def grad(self, y, h):\n",
    "        return y - h\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        self.models_trained = []\n",
    "        first_model = DummyRegressor(strategy='mean')\n",
    "        first_model.fit(X, y)\n",
    "        self.models_trained.append(first_model)\n",
    "        \n",
    "        for i, model in enumerate(self.models):\n",
    "            y_pred = self.predict(X, with_argmax=False)\n",
    "            residual = self.grad(y, y_pred)\n",
    "            model.fit(X, residual)\n",
    "            self.models_trained.append(model)\n",
    "    \n",
    "    def predict(self, X, with_argmax=True):\n",
    "        models = self.models_trained\n",
    "        f0 = models[0].predict(X)\n",
    "        boosting = sum(self.learning_rate * model.predict(X) for model in models[1:])\n",
    "        yhat = f0 + boosting\n",
    "        if not self.regression:\n",
    "            yhat = np.exp(yhat) / np.sum(np.exp(yhat), axis=1, keepdims=True)\n",
    "            if with_argmax:\n",
    "                yhat = np.argmax(yhat, axis=1)\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5805241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  7.803138676200792\n",
      "Sklearn MSE:  7.7902844330749765\n"
     ]
    }
   ],
   "source": [
    "#regression\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y = load_boston(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                        test_size=0.3, random_state=42)\n",
    "\n",
    "model = GradientBoosting(n_estimators=200, learning_rate=0.1, max_depth = 3, \n",
    "                 min_samples_split = 2,\n",
    "                 regression=True)\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "#print metrics\n",
    "print(\"MSE: \", mean_squared_error(y_test, yhat))\n",
    "\n",
    "n_estimators = 200\n",
    "\n",
    "#=====SKlearn========\n",
    "#Compare to sklearn: ls is the same as our mse\n",
    "sklearn_model = GradientBoostingRegressor(\n",
    "    n_estimators=n_estimators,\n",
    "    learning_rate = 0.1,\n",
    "    max_depth=3,\n",
    "    loss='ls'\n",
    ")\n",
    "\n",
    "yhat_sk = sklearn_model.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Sklearn MSE: \", mean_squared_error(y_test, yhat_sk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c2b4b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our accuracy:  0.9649122807017544\n",
      "Sklearn accuracy:  0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "# Binary classification\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "y_train_encoded = np.zeros((y_train.shape[0], len(set(y))))\n",
    "for each_class in range(len(set(y))):\n",
    "    cond = y_train==each_class\n",
    "    y_train_encoded[np.where(cond), each_class] = 1\n",
    "\n",
    "model = GradientBoosting(n_estimators=200, learning_rate=0.1, max_depth = 3, \n",
    "                 min_samples_split = 2,\n",
    "                 regression=False)\n",
    "model.fit(X_train, y_train_encoded)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# #print metrics\n",
    "print(\"Our accuracy: \", accuracy_score(y_test, yhat))\n",
    "\n",
    "#=====SKlearn========\n",
    "#Compare to sklearn: ls is the same as our accuracy\n",
    "sklearn_model = GradientBoostingClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    learning_rate = 0.1,\n",
    "    max_depth=1\n",
    ")\n",
    "\n",
    "yhat_sk = sklearn_model.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Sklearn accuracy: \", accuracy_score(y_test, yhat_sk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be4a49ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our accuracy:  0.9314814814814815\n",
      "Sklearn accuracy:  0.9481481481481482\n"
     ]
    }
   ],
   "source": [
    "# Multiclass classification\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "y_train_encoded = np.zeros((y_train.shape[0], len(set(y))))\n",
    "for each_class in range(len(set(y))):\n",
    "    cond = y_train==each_class\n",
    "    y_train_encoded[np.where(cond), each_class] = 1\n",
    "\n",
    "model = GradientBoosting(n_estimators=200, learning_rate=0.1, max_depth = 3, \n",
    "                 min_samples_split = 2,\n",
    "                 regression=False)\n",
    "model.fit(X_train, y_train_encoded)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# #print metrics\n",
    "print(\"Our accuracy: \", accuracy_score(y_test, yhat))\n",
    "\n",
    "#=====SKlearn========\n",
    "#Compare to sklearn: ls is the same as our accuracy\n",
    "sklearn_model = GradientBoostingClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    learning_rate = 0.1,\n",
    "    max_depth=1\n",
    ")\n",
    "\n",
    "yhat_sk = sklearn_model.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Sklearn accuracy: \", accuracy_score(y_test, yhat_sk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0314f44e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
